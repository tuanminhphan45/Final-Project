model:
  arch: blip2_timesformer
  model_type: pretrain
  vit_model: timesformer
  img_size: 224
  num_frames: 16
  drop_path_rate: 0.1
  use_grad_checkpointing: false
  vit_precision: fp16
  freeze_vit: true
  num_query_token: 128
  cross_attention_freq: 2
  embed_dim: 768
  max_txt_len: 32
  timesformer_model_type: alpro
  timesformer_pretrained: https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained.pth
  qformer_pretrained: "/storage/student10/vidcaption/LAVIS/lavis/output/BLIP2_TimeSformer/pretrain_stage1/20250513063/checkpoint_9.pth"

datasets:
  msrvtt:  # name of the dataset builder
    vis_processor:
        eval:
          name: "alpro_video_eval"
          n_frms: 16
          image_size: 224
    text_processor:
        eval:
          name: "blip_caption"

run:
  task: video_captioning
  # optimization-specific
  batch_size_train: 32
  batch_size_eval: 64
  num_workers: 4

  seed: 42
  output_dir: "output/blip2_timesformer/caption_msrvtt_eval"

  evaluate: True
  valid_splits: ["val"]
  test_splits: ["test"]

  # distribution-specific
  device: "cuda"
  world_size: 4
  dist_url: "env://"
  distributed: True
  amp: true
  resume: false

  # checkpoint configuration
  resume_ckpt_path: "/storage/student10/vidcaption/LAVIS/lavis/output/BLIP2_TimeSformer/pretrain_stage1/20250513063/checkpoint_9.pth"
  eval_ckpt_path: "/storage/student10/vidcaption/LAVIS/lavis/output/BLIP2_TimeSformer/pretrain_stage1/20250513063/checkpoint_9.pth"

evaluation:
  caption:
    method: beam
    num_beams: 5
    max_length: 30
    min_length: 5
    repetition_penalty: 1.0
    length_penalty: 1.0 